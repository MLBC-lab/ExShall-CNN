{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e1ca20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this notebook, we will combine Conv2D with\n",
    "# logarithmic and exponential functions to assimilate\n",
    "# different type of kernels, such as Laplace, RBF, \n",
    "# Multipying, Deviding, etc.\n",
    "# for \"Retina Blood Vessel\" dataset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf2be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernels\n",
    "# \n",
    "# exp(log(x) + log(y)) = x * y\n",
    "# exp(log(x) - log(y)) = x / y\n",
    "# exp(-|x - y|) = \n",
    "#     x > y -> exp(-x + y) = exp(-x) * exp(y)\n",
    "#     x < y -> exp(x - y) = exp(x) * exp(y)\n",
    "# exp(log(|x|) + log(|y|)) = |x| * |y|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f64d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "#\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import skimage\n",
    "from skimage import segmentation, io, filters, morphology\n",
    "import sklearn\n",
    "from sklearn import ensemble, metrics, svm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.subplots\n",
    "import plotly.express as px\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "import torch, torchvision\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18c6227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "#\n",
    "train_image_ipath = 'RetinaBloodVessels/train/image/'\n",
    "train_mask_ipath = 'RetinaBloodVessels/train/mask/'\n",
    "test_image_ipath = 'RetinaBloodVessels/test/image/'\n",
    "test_mask_ipath = 'RetinaBloodVessels/test/mask/'\n",
    "NROWS, NCOLS = 512, 512\n",
    "EPSILON = 1e-6\n",
    "br = set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2de1972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 512, 512, 3) (80, 512, 512) (20, 512, 512, 3) (20, 512, 512)\n",
      "-1.5989004704853944 4.097728417232479 1.5034270125132329e-18\n",
      "0 1 0.12315158843994141\n"
     ]
    }
   ],
   "source": [
    "# read all images and masks and store in two matrices\n",
    "train_images = read_images(train_image_ipath)\n",
    "train_masks = 1 * (read_images(train_mask_ipath, rescale=False) > 0)\n",
    "test_images = read_images(test_image_ipath)\n",
    "test_masks = 1 *(read_images(test_mask_ipath, rescale=False) > 0)\n",
    "print(train_images.shape, train_masks.shape,\n",
    "      test_images.shape, test_masks.shape)\n",
    "print(train_images.min(), train_images.max(), train_images.mean())\n",
    "print(train_masks.min(), train_masks.max(), train_masks.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "081c80e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions and classes\n",
    "\n",
    "def read_images(path, rescale=True):\n",
    "    images_fnames = sorted(glob.glob(os.path.join(path, '*.png')))\n",
    "    images = []\n",
    "    for fn in images_fnames:\n",
    "        img = io.imread(fn)\n",
    "        if rescale:\n",
    "            img = np.float64(img)\n",
    "            # Min-Max\n",
    "            # img = (img - img.min()) / (img.max() + EPSILON)\n",
    "            # img = 2*img - 1\n",
    "            # img = np.float64(img)/img.max()\n",
    "            # Normalization\n",
    "            img = (img - img.mean()) / img.std()\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images\n",
    "\n",
    "def gray(img):\n",
    "    gr = img.mean(axis=2)\n",
    "    gr = (gr - gr.min()) / (gr.max() - gr.min() + EPSILON)\n",
    "    return gr\n",
    "\n",
    "def show(img):\n",
    "    if img.max != 255:\n",
    "        img = np.float64(img)\n",
    "        img = np.uint8(255*(img - img.min())/(img.max()-img.min() + EPSILON))\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.subplots()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    return True\n",
    "\n",
    "class Poly(torch.nn.Module):\n",
    "    def __init__(self, degree):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "    def extra_repr(self):\n",
    "        return f'{self.degree}'\n",
    "    def forward(self, x):\n",
    "        out = x**self.degree\n",
    "        if out.isnan().any() | out.isinf().any():\n",
    "            br()\n",
    "        return out\n",
    "\n",
    "class Concat(torch.nn.Module):\n",
    "    def __init__(self, ops: list = []):\n",
    "        super().__init__()\n",
    "        self.ops = torch.nn.ModuleList()\n",
    "        self.ops += ops\n",
    "    def append(self, op):\n",
    "        self.ops.append(op)\n",
    "        return self\n",
    "    def forward(self, x):\n",
    "        # br()\n",
    "        comb = [op(x) for op in self.ops]\n",
    "        out = torch.concatenate(comb, dim=1)\n",
    "        if out.isnan().any() | out.isinf().any():\n",
    "            print(self)\n",
    "            br()\n",
    "        return out\n",
    "\n",
    "class Exp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Exp, self).__init__()\n",
    "    def forward(self, x):\n",
    "        out = torch.exp(x)\n",
    "        if out.isnan().any() | out.isinf().any():\n",
    "            print(self)\n",
    "            br()\n",
    "        return out\n",
    "\n",
    "class Log(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Log, self).__init__()\n",
    "    def forward(self, x):\n",
    "        out = torch.log(x.abs() + EPSILON)\n",
    "        if out.isnan().any() | out.isinf().any():\n",
    "            print(self)\n",
    "            br()\n",
    "        return out\n",
    "\n",
    "class Laplacian(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RBF's Kernel between chunks of nch channel\n",
    "    \"\"\"\n",
    "    def __init__(self, nch):\n",
    "        super().__init__()\n",
    "        self.nch = nch\n",
    "        self.epsilon = EPSILON\n",
    "        self.slope = torch.nn.Parameter(torch.randn(1),\n",
    "                                        requires_grad=True)\n",
    "    def extra_repr(self):\n",
    "        return (f'{self.nch}, ' +\n",
    "                f'{self.slope.item():.4f}')\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: should be (batch, channel, ...)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            while (((x + self.epsilon) == 0.0).any() | \n",
    "                   (self.epsilon < 1e-8)):\n",
    "                self.epsilon = EPSILON * torch.randn(1).item()\n",
    "        out = []\n",
    "        for c1 in range(0, x.shape[1] // self.nch - 1):\n",
    "            for c2 in range(c1+1, x.shape[1] // self.nch):\n",
    "                slc1 = slice(c1*self.nch, (c1+1)*self.nch)\n",
    "                slc2 = slice(c2*self.nch, (c2+1)*self.nch)\n",
    "                rbf = torch.exp(\n",
    "                    -(self.slope*(x[:, slc1, :, :]-x[:, slc2, :, :])).abs())\n",
    "                out.append(rbf)\n",
    "        out = torch.concatenate(out, dim=1)\n",
    "        if out.isnan().any() | out.isinf().any():\n",
    "            print(self)\n",
    "            br()\n",
    "        return out\n",
    "\n",
    "class JaccardLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1, weight=None, size_average=True):\n",
    "        super(JaccardLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        smooth = self.smooth\n",
    "\n",
    "        # #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        # inputs = inputs.view(-1)\n",
    "        # targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        jac = (intersection + smooth)/(union + smooth)\n",
    "        \n",
    "        return -torch.log(jac)\n",
    "        # return 1 - jac\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        smooth = self.smooth\n",
    "\n",
    "        # #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        # flatten label and prediction tensors\n",
    "        # inputs = inputs.view(-1)\n",
    "        # targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        return -torch.log(dice)\n",
    "        # return 1 - dice\n",
    "\n",
    "# Dice Binary Cross Entropy Coefficient\n",
    "class DiceBCELoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        # #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        # inputs = inputs.view(-1)\n",
    "        # targets = targets.view(-1)\n",
    "        inputs, targets = inputs.flatten(), targets.flatten()\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = torch.nn.functional.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE\n",
    "    \n",
    "class Train:\n",
    "    \"\"\"\n",
    "    Initialize, train, evaluate, decode\n",
    "    \"\"\"\n",
    "    def __init__(self, model, data, label, nepoch=4, bsize=8):\n",
    "        \"\"\"\n",
    "        data: [batch, num_channel, rows, cols]\n",
    "        label: [batch, num_channel, rows, cols]\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.nepoch = nepoch\n",
    "        self.device = (torch.device(\"cuda:0\")\n",
    "                       if torch.cuda.is_available()\n",
    "                       else torch.device('cpu'))\n",
    "        print('Device: ', self.device)\n",
    "        # weight = torch.Tensor([label.sum()/label.numel(),\n",
    "        #                        1-label.sum()/label.numel()])\n",
    "        # print('Weight: ', weight)\n",
    "        # self.crit = torch.nn.CrossEntropyLoss(weight=weight.to(self.device))\n",
    "        # self.crit = DiceLoss(smooth=0.0)\n",
    "        # self.crit = DiceBCELoss()\n",
    "        self.crit = JaccardLoss(smooth=0.0)\n",
    "        self.bsize = bsize\n",
    "    def run(self, lr=1e-4):\n",
    "        bmodel, bloss = self.model, float('inf')\n",
    "        model = self.model\n",
    "        model = model.to(self.device)\n",
    "        crit = self.crit\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optim, factor=0.5, patience=4, threshold=0.001)\n",
    "        t1 = time.time()\n",
    "        for epoch in range(self.nepoch):\n",
    "            # t1 = time.time()\n",
    "            model, optim = self._train(model, crit, optim)\n",
    "            \n",
    "            loss = self._valid(model, crit)\n",
    "            # sch.step(loss)\n",
    "            # if lr != optim.param_groups[0]['lr']:\n",
    "            #     lr = optim.param_groups[0]['lr']\n",
    "            #     print(f'Learning rate changed to {lr:.04f}.')\n",
    "            if (epoch % 10 == 0) | (epoch == (self.nepoch-1)):\n",
    "                print(f'Ep: {epoch+1}, Secs: {time.time() - t1:.0f}, ' +\n",
    "                      f'loss: {loss:.04f}')\n",
    "                t1 = time.time()\n",
    "            if loss < bloss:\n",
    "                bmodel, bloss = copy.deepcopy(model), loss\n",
    "        return bmodel, bloss\n",
    "            \n",
    "    def _train(self, model, crit, optim):\n",
    "        model.train()\n",
    "        for bc in range(1, self.data.shape[0] // self.bsize + 1 +\n",
    "                        1*(self.data.shape[0] % self.bsize != 0)):\n",
    "            slc = slice((bc-1)*self.bsize, bc*self.bsize)\n",
    "            batch, lb = self.data[slc, :, :, :], self.label[slc].long()\n",
    "            batch, lb = batch.to(self.device), lb.to(self.device)\n",
    "            optim.zero_grad()\n",
    "            out = model(batch)\n",
    "            # out = out.softmax(dim=1)\n",
    "            # loss = crit(out, lb)\n",
    "            # loss = crit(out.swapaxes(1, 2).swapaxes(2, 3).flatten(0, 2),\n",
    "            #             lb.flatten())\n",
    "            # loss = crit(out[:, 1, :, :], lb.float())\n",
    "            loss = crit(out, lb.float())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        return model, optim\n",
    "    \n",
    "    def _valid(self, model, crit):\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for bc in range(1, self.data.shape[0] // self.bsize + 1 +\n",
    "                            1*(self.data.shape[0] % self.bsize != 0)):\n",
    "                slc = slice((bc-1)*self.bsize, bc*self.bsize)\n",
    "                batch, lb = self.data[slc], self.label[slc]\n",
    "                batch, lb = batch.to(self.device), lb.to(self.device).long()\n",
    "                out = model(batch)\n",
    "                # out = out.softmax(dim=1)\n",
    "                # loss = crit(out, lb)\n",
    "                # loss = crit(out.swapaxes(1, 2).swapaxes(2, 3).flatten(0, 2),\n",
    "                #             lb.flatten())\n",
    "                loss = crit(out, lb.float())\n",
    "                loss_sum += loss.item()\n",
    "        return loss_sum / bc\n",
    "\n",
    "    def decode(self, model):\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        decs = []\n",
    "        with torch.no_grad():\n",
    "            for bc in range(1, self.data.shape[0] // self.bsize + 1 +\n",
    "                            1*(self.data.shape[0] % self.bsize != 0)):\n",
    "                slc = slice((bc-1)*self.bsize, bc*self.bsize)\n",
    "                batch, lb = self.data[slc], self.label[slc]\n",
    "                batch, lb = batch.to(self.device), lb.to(self.device).long()\n",
    "                out = model(batch)\n",
    "                # out = out.softmax(dim=1)\n",
    "                decs += out.detach().cpu().tolist()\n",
    "        decs = np.array(decs)\n",
    "        return decs\n",
    "\n",
    "def decode(model, data, label, batch_size, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    decs = []\n",
    "    with torch.no_grad():\n",
    "        for bc in range(1, data.shape[0] // batch_size + 1 +\n",
    "                        1*(data.shape[0] % batch_size != 0)):\n",
    "            slc = slice((bc-1)*batch_size, bc*batch_size)\n",
    "            batch, lb = data[slc], label[slc]\n",
    "            batch, lb = batch.to(device), lb.to(device).long()\n",
    "            out = model(batch)\n",
    "            decs += out.detach().cpu().tolist()\n",
    "    decs = np.array(decs)\n",
    "    return decs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae75e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Nonlinear Block\n",
    "class LocallyNonlinear(torch.nn.Module):\n",
    "    def __init__(self, neighbor_radius=[3], dilation=[1],\n",
    "                 ochannel=3, drout=0.0):\n",
    "        super().__init__()\n",
    "        neighbor = [2*nr+1 for nr in neighbor_radius]\n",
    "        self.neighbor = neighbor\n",
    "        ln = len(neighbor)\n",
    "        # chunk_size = ochannel if degree > 1 else 1\n",
    "        self.layers = Concat([torch.nn.Identity()])\n",
    "        for nei, dil in zip(neighbor, dilation):\n",
    "            layer = Concat()\n",
    "            # Coefficient multiplier\n",
    "            layer.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3, \n",
    "                                out_channels=ochannel,\n",
    "                                kernel_size=nei, stride=1,\n",
    "                                dilation=dil, padding='same',\n",
    "                                bias=True)))\n",
    "            # Summation of coefficient multiplier\n",
    "            layer.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3, \n",
    "                                out_channels=ochannel,\n",
    "                                kernel_size=nei, stride=1,\n",
    "                                dilation=dil, padding='same',\n",
    "                                bias=True),\n",
    "                torch.nn.Conv2d(in_channels=ochannel, \n",
    "                                out_channels=ochannel,\n",
    "                                kernel_size=nei, stride=1,\n",
    "                                dilation=dil, padding='same',\n",
    "                                bias=True)))\n",
    "            # ReLU(ReLU)\n",
    "            layer.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3, \n",
    "                                out_channels=ochannel,\n",
    "                                kernel_size=nei, stride=1,\n",
    "                                dilation=dil, padding='same',\n",
    "                                bias=True), \n",
    "                torch.nn.ReLU(), \n",
    "                torch.nn.Conv2d(in_channels=ochannel,\n",
    "                                out_channels=ochannel, \n",
    "                                kernel_size=1, stride=1,\n",
    "                                dilation=1, padding='same',\n",
    "                                bias=True),\n",
    "                torch.nn.ReLU()))\n",
    "            self.layers.append(layer) \n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(4, 3, 5, 5)\n",
    "            comb = self.layers(x)\n",
    "        self.aggregate = torch.nn.Sequential(\n",
    "            torch.nn.Dropout2d(p=drout),\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=comb.shape[1],\n",
    "                out_channels=ochannel, \n",
    "                kernel_size=1, stride=1, \n",
    "                padding='same', bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout2d(p=drout),\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=ochannel,\n",
    "                out_channels=1, \n",
    "                kernel_size=1, stride=1, \n",
    "                padding='same', bias=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        if y.isnan().any() | y.isinf().any():\n",
    "            br()\n",
    "        y = self.aggregate(y)\n",
    "        if y.isnan().any() | y.isinf().any():\n",
    "            br()\n",
    "        y = y.squeeze(dim=1).sigmoid()\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371f26f-3677-4cce-a583-cecba36cfb04",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "Ep: 1, Secs: 15, loss: 1.6876\n",
      "Ep: 11, Secs: 144, loss: 1.6813\n",
      "Ep: 21, Secs: 144, loss: 1.6802\n",
      "Ep: 31, Secs: 144, loss: 1.6778\n",
      "Ep: 41, Secs: 144, loss: 1.0789\n",
      "Ep: 51, Secs: 144, loss: 0.7107\n",
      "Ep: 61, Secs: 144, loss: 0.6921\n",
      "Ep: 71, Secs: 144, loss: 0.6897\n",
      "Ep: 81, Secs: 144, loss: 0.6727\n",
      "Ep: 91, Secs: 144, loss: 0.6678\n",
      "Ep: 101, Secs: 144, loss: 0.6817\n",
      "Ep: 111, Secs: 144, loss: 0.6444\n",
      "Ep: 121, Secs: 144, loss: 0.6758\n",
      "Ep: 131, Secs: 144, loss: 0.6369\n",
      "Ep: 141, Secs: 144, loss: 0.6513\n",
      "Ep: 151, Secs: 144, loss: 0.6255\n",
      "Ep: 161, Secs: 144, loss: 0.6358\n",
      "Ep: 171, Secs: 144, loss: 0.6064\n",
      "Ep: 181, Secs: 144, loss: 0.6079\n",
      "Ep: 191, Secs: 144, loss: 0.5911\n",
      "Ep: 201, Secs: 144, loss: 0.5904\n",
      "Ep: 211, Secs: 144, loss: 0.5762\n",
      "Ep: 221, Secs: 144, loss: 0.5638\n",
      "Ep: 231, Secs: 144, loss: 0.5484\n",
      "Ep: 241, Secs: 144, loss: 0.5323\n",
      "Ep: 251, Secs: 144, loss: 0.5272\n",
      "Ep: 261, Secs: 144, loss: 0.5243\n",
      "Ep: 271, Secs: 144, loss: 0.5187\n",
      "Ep: 281, Secs: 144, loss: 0.5119\n",
      "Ep: 291, Secs: 144, loss: 0.5107\n",
      "Ep: 301, Secs: 144, loss: 0.5023\n",
      "Ep: 311, Secs: 144, loss: 0.4926\n",
      "Ep: 321, Secs: 144, loss: 0.4859\n",
      "Ep: 331, Secs: 144, loss: 0.4762\n",
      "Ep: 341, Secs: 144, loss: 0.4736\n",
      "Ep: 351, Secs: 144, loss: 0.4652\n",
      "Ep: 361, Secs: 144, loss: 0.4628\n",
      "Ep: 371, Secs: 144, loss: 0.4612\n",
      "Ep: 381, Secs: 144, loss: 0.4662\n",
      "Ep: 391, Secs: 144, loss: 0.4476\n",
      "Ep: 401, Secs: 144, loss: 0.4517\n",
      "Ep: 411, Secs: 144, loss: 0.4403\n",
      "Ep: 421, Secs: 144, loss: 0.4350\n",
      "Ep: 431, Secs: 144, loss: 0.4347\n",
      "Ep: 441, Secs: 144, loss: 0.4333\n",
      "Ep: 451, Secs: 144, loss: 0.4258\n",
      "Ep: 461, Secs: 144, loss: 0.4257\n",
      "Ep: 471, Secs: 144, loss: 0.4227\n",
      "Ep: 481, Secs: 144, loss: 0.4146\n",
      "Ep: 491, Secs: 144, loss: 0.4146\n",
      "Ep: 501, Secs: 144, loss: 0.4104\n",
      "Ep: 511, Secs: 144, loss: 0.5738\n",
      "Ep: 521, Secs: 144, loss: 0.4433\n",
      "Ep: 531, Secs: 144, loss: 0.4219\n",
      "Ep: 541, Secs: 144, loss: 0.4170\n",
      "Ep: 551, Secs: 144, loss: 0.4099\n",
      "Ep: 561, Secs: 144, loss: 0.4064\n",
      "Ep: 571, Secs: 144, loss: 0.4032\n",
      "Ep: 581, Secs: 144, loss: 0.4022\n",
      "Ep: 591, Secs: 144, loss: 0.3999\n",
      "Ep: 601, Secs: 144, loss: 0.3988\n",
      "Ep: 611, Secs: 144, loss: 0.3980\n",
      "Ep: 621, Secs: 144, loss: 0.3965\n",
      "Ep: 631, Secs: 144, loss: 0.3925\n",
      "Ep: 641, Secs: 144, loss: 0.3941\n",
      "Ep: 651, Secs: 144, loss: 0.3894\n",
      "Ep: 661, Secs: 144, loss: 0.3913\n",
      "Ep: 671, Secs: 144, loss: 0.3873\n",
      "Ep: 681, Secs: 144, loss: 0.3856\n",
      "Ep: 691, Secs: 144, loss: 0.3862\n",
      "Ep: 701, Secs: 144, loss: 0.3867\n",
      "Ep: 711, Secs: 144, loss: 0.3811\n",
      "Ep: 721, Secs: 144, loss: 0.3821\n",
      "Ep: 731, Secs: 144, loss: 0.3831\n",
      "Ep: 741, Secs: 144, loss: 0.3796\n",
      "Ep: 751, Secs: 144, loss: 0.3784\n",
      "Ep: 761, Secs: 144, loss: 0.3755\n",
      "Ep: 771, Secs: 144, loss: 0.3828\n",
      "Ep: 781, Secs: 144, loss: 0.3719\n",
      "Ep: 791, Secs: 144, loss: 0.3698\n",
      "Ep: 801, Secs: 144, loss: 0.3708\n",
      "Ep: 811, Secs: 144, loss: 0.3766\n",
      "Ep: 821, Secs: 144, loss: 0.3699\n",
      "Ep: 831, Secs: 144, loss: 0.3725\n",
      "Ep: 841, Secs: 144, loss: 0.3736\n",
      "Ep: 851, Secs: 144, loss: 0.3697\n",
      "Ep: 861, Secs: 144, loss: 0.3674\n",
      "Ep: 871, Secs: 144, loss: 0.3641\n",
      "Ep: 881, Secs: 144, loss: 0.3660\n",
      "Ep: 891, Secs: 144, loss: 0.3603\n",
      "Ep: 901, Secs: 144, loss: 0.3597\n",
      "Ep: 911, Secs: 144, loss: 0.3657\n",
      "Ep: 921, Secs: 144, loss: 0.3611\n",
      "Ep: 931, Secs: 144, loss: 0.3631\n"
     ]
    }
   ],
   "source": [
    "nl = LocallyNonlinear(\n",
    "    neighbor_radius = [0, 1, 2, 3, 6, 7, 8],\n",
    "    dilation        = [1, 1, 2, 3, 4, 5, 6],\n",
    "    ochannel=64, drout=0.1)\n",
    "# print(nl)\n",
    "# shift channel to the second position\n",
    "train_images_tensors = torch.Tensor(train_images).swapdims(2, 3).swapdims(1, 2)\n",
    "test_images_tensors = torch.Tensor(test_images).swapdims(2, 3).swapdims(1, 2)\n",
    "train = Train(nl, train_images_tensors,\n",
    "              torch.Tensor(train_masks), nepoch=1000, bsize=5)\n",
    "t0 = time.time()\n",
    "model, loss = train.run(lr=1.0e-3)\n",
    "print(f'Best loss is {loss:.4f}.')\n",
    "\n",
    "# Jaccard, Dice, etc. on Train and Test\n",
    "#\n",
    "trdecs = decode(model, train_images_tensors, torch.Tensor(train_masks),\n",
    "                5, torch.device('cuda:0'))\n",
    "tsdecs = decode(model, test_images_tensors, torch.Tensor(test_masks),\n",
    "                5, torch.device('cuda:0'))\n",
    "# pr = 1.0 * (decs > 1.5*decs.mean()).flatten()\n",
    "for decs, label in zip([trdecs, tsdecs], [train_masks, test_masks]):\n",
    "    pr = 1.0 * (decs > 0.5*decs.mean()).flatten()\n",
    "    tg = label.flatten()\n",
    "    jaccard = sklearn.metrics.jaccard_score(tg, pr)\n",
    "    dice = 2 * jaccard / (jaccard + 1)\n",
    "    recall = sklearn.metrics.recall_score(tg, pr)\n",
    "    precision = sklearn.metrics.precision_score(tg, pr)\n",
    "    f1score = sklearn.metrics.f1_score(tg, pr)\n",
    "    print(f'Jaccard: {jaccard:.4f}, Dice: {dice:.4f}, ' + \n",
    "          f'Recall: {recall:.4f}, Precision: {precision:.4f}, ' +\n",
    "          f'F1-score: {f1score:.4f}')\n",
    "\n",
    "print(f'Finished in {time.time() - t0:.0f} seconds.')\n",
    "\n",
    "# plot an example\n",
    "n = 10\n",
    "show(decs[n, :, :])\n",
    "# show(1.0*(decs[n, 0, :, :] < decs[n, 1, :, :]))\n",
    "show(train_masks[n, :, :])\n",
    "show(train_images[n, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ec661",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decs.min(), decs.max(), decs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64c0df-c461-4e90-ac46-51492020e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "# show(decs[n, 1, :, :])\n",
    "# show(1.0*(decs[n, 0, :, :] < decs[n, 1, :, :]))\n",
    "show(decs[n, :, :])\n",
    "show(train_masks[n, :, :])\n",
    "show(train_images[n, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LocallyNonlinear(\n",
    "#     neighbor_radius = [0, 1, 2, 3, 6, 7],\n",
    "#     dilation        = [1, 1, 2, 3, 4, 5],\n",
    "#     ochannel=32, drout=0.1)\n",
    "\n",
    "# Ep: 1000, Secs: 47, loss: 0.3903\n",
    "# Best loss is 0.3886.\n",
    "# 0.6793215968467662 0.8090428874639818\n",
    "# Finished in 5233 seconds.\n",
    "# Jaccard: 0.6791, Dice: 0.8089, Recall: 0.7898, Precision: 0.8289, F1-score: 0.8089\n",
    "# Jaccard: 0.6546, Dice: 0.7913, Recall: 0.7645, Precision: 0.8200, F1-score: 0.7913"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myjupyter",
   "language": "python",
   "name": "myjupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
